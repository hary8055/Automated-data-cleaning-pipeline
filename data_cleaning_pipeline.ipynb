{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6657673-94fd-473b-9321-f79a3f61508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747ad059-ac97-46c6-bd0e-ead950976568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaningPipeline:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.df = None\n",
    "\n",
    "    def load_data(self):\n",
    "        #Load dataset\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.data_path)\n",
    "            print(f\"Data loaded successfully with {self.df.shape[0]} rows and {self.df.shape[1]} columns.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def schema_validation(self, expected_columns):\n",
    "        #Checking expected columns\n",
    "        missing_columns = set(expected_columns) - set(self.df.columns)\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing columns: {missing_columns}\")\n",
    "        print(\"validation passed.\")\n",
    "\n",
    "    def handle_missing_data(self):\n",
    "        # Dropping columns where more than 50% of data is missing\n",
    "        self.df = self.df.dropna(thresh=self.df.shape[0]*0.5, axis=1)\n",
    "        \n",
    "        # Imputing missing values\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        numeric_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        self.df[numeric_cols] = imputer.fit_transform(self.df[numeric_cols])\n",
    "        print(f\"Missing data handled. Remaining columns with NaNs: {self.df.isna().sum().sum()}\")\n",
    "\n",
    "    def handle_duplicates(self):\n",
    "        #Remove duplicates\n",
    "        before = self.df.shape[0]\n",
    "        self.df = self.df.drop_duplicates()\n",
    "        after = self.df.shape[0]\n",
    "        print(f\"Removed {before - after} duplicate rows.\")\n",
    "        \n",
    "    def detect_outliers(self, z_thresh=3):\n",
    "        #handle outlier (Z-score)\n",
    "        numeric_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        z_scores = np.abs((self.df[numeric_cols] - self.df[numeric_cols].mean()) / self.df[numeric_cols].std())\n",
    "        self.df = self.df[(z_scores < z_thresh).all(axis=1)]\n",
    "        print(f\"Outliers removed, {self.df.shape[0]} rows remaining.\")\n",
    "\n",
    "    def clean_strings(self):\n",
    "        #further cleaning (e.g., trim, lowercase, remove special chars).\n",
    "        string_cols = self.df.select_dtypes(include='object').columns\n",
    "        for col in string_cols:\n",
    "            self.df[col] = self.df[col].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x).lower().strip()))\n",
    "        print(f\"String columns cleaned.\")\n",
    "\n",
    "    def data_type_conversion(self):\n",
    "        # Convert 'date' columns to datetime\n",
    "        date_cols = [col for col in self.df.columns if 'date' in col.lower()]\n",
    "        for col in date_cols:\n",
    "            self.df[col] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "        \n",
    "        # Convert categorical columns to 'category' type\n",
    "        categorical_cols = self.df.select_dtypes(include='object').columns\n",
    "        for col in categorical_cols:\n",
    "            self.df[col] = self.df[col].astype('category')\n",
    "        \n",
    "        print(f\"Data type conversion completed.\")\n",
    "    \n",
    "    def encode_and_scale(self):\n",
    "        # One-hot encoding for categorical variables\n",
    "        categorical_cols = self.df.select_dtypes(include='category').columns\n",
    "        self.df = pd.get_dummies(self.df, columns=categorical_cols, drop_first=True)\n",
    "        \n",
    "        # Scaling numerical columns\n",
    "        numeric_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        scaler = StandardScaler()\n",
    "        self.df[numeric_cols] = scaler.fit_transform(self.df[numeric_cols])\n",
    "        print(f\"Data encoding and scaling completed.\")\n",
    "    \n",
    "    def validate_data(self):\n",
    "        #Final validation\n",
    "        if self.df.isnull().sum().sum() > 0:\n",
    "            raise ValueError(\"Data contains null values after cleaning.\")\n",
    "        \n",
    "        # Ensure no negative or impossible values in specific columns\n",
    "        if 'age' in self.df.columns:\n",
    "            if (self.df['age'] < 0).any():\n",
    "                raise ValueError(\"Age contains negative values.\")\n",
    "        \n",
    "        print(\"Data validation passed.\")\n",
    "    \n",
    "    def save_cleaned_data(self, output_path):\n",
    "        self.df.to_csv(output_path, index=False)\n",
    "        print(f\"Cleaned data saved to {output_path}.\")\n",
    "    \n",
    "    def run_pipeline(self, expected_columns, output_path):\n",
    "        self.load_data()\n",
    "        self.schema_validation(expected_columns)\n",
    "        self.handle_missing_data()\n",
    "        self.handle_duplicates()\n",
    "        self.detect_outliers()\n",
    "        self.clean_strings()\n",
    "        self.data_type_conversion()\n",
    "        self.encode_and_scale()\n",
    "        self.validate_data()\n",
    "        self.save_cleaned_data(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8252af94-9c52-45f4-8ca4-dfdeed840066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully with 25000 rows and 17 columns.\n",
      "validation passed.\n",
      "Missing data handled. Remaining columns with NaNs: 0\n",
      "Removed 0 duplicate rows.\n",
      "Outliers removed, 23121 rows remaining.\n",
      "String columns cleaned.\n",
      "Data type conversion completed.\n",
      "Data encoding and scaling completed.\n",
      "Data validation passed.\n",
      "Cleaned data saved to /Users/harikrishnans/Downloads/cleaned_data.csv.\n"
     ]
    }
   ],
   "source": [
    "#usage\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = '/Users/harikrishnans/Downloads/hospital_readmissions.csv'  \n",
    "    output_path = '/Users/harikrishnans/Downloads/cleaned_data.csv'\n",
    "    \n",
    "    expected_columns = ['age', 'time_in_hospital', 'n_lab_procedures', 'n_procedures',\n",
    "       'n_medications', 'n_outpatient', 'n_inpatient', 'n_emergency',\n",
    "       'medical_specialty', 'diag_1', 'diag_2', 'diag_3', 'glucose_test',\n",
    "       'A1Ctest', 'change', 'diabetes_med', 'readmitted']  # Define expected columns\n",
    "\n",
    "    pipeline = DataCleaningPipeline(data_path)\n",
    "    pipeline.run_pipeline(expected_columns, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeca089-04df-4d4a-9445-27710472e527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
